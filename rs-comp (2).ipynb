{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12486024,"sourceType":"datasetVersion","datasetId":7878862}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install polars","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import polars as pl\nimport torch\nfrom PIL import Image\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom tqdm import tqdm\nfrom transformers import CLIPProcessor, CLIPModel\n\nclass ItemEncoder:\n    def __init__(self,\n                device,\n                encoder_model_name: str = \"openai/clip-vit-base-patch32\",\n    ):\n        self.device = device\n        self.model = CLIPModel.from_pretrained(encoder_model_name)\n        self.processor = CLIPProcessor.from_pretrained(encoder_model_name)\n\n        self.model.to(self.device)\n\n        for param in self.model.parameters():\n            param.requires_grad = False\n        self.model.eval()\n    \n    @torch.no_grad()\n    def encode_items(self, images, titles):\n        \"\"\"\n        Args:\n            images: PIL Image or list of PIL Images\n            titles: str or list of str\n        Returns:\n            torch.Tensor: item embeddings [batch_size, hidden_dim]\n        \"\"\"\n        # IMPORTANT: Add truncation=True and max_length=77\n        inputs = self.processor(\n            text=titles, \n            images=images, \n            return_tensors=\"pt\", \n            padding=True,\n            truncation=True,  # Truncate long texts\n            max_length=77     # CLIP's max sequence length\n        )\n        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n        \n        outputs = self.model(**inputs)\n        image_embeds = outputs.image_embeds  # [batch_size, 512]\n        text_embeds = outputs.text_embeds     # [batch_size, 512]\n        \n        # Combine them (concatenate)\n        item_embedding = torch.cat([image_embeds, text_embeds], dim=1)  # [batch_size, 1024]\n        \n        return item_embedding\n\n\ndef main():\n    # Setup device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    # Load data\n    print(\"Loading data...\")\n    items_features = pl.read_parquet(\"/kaggle/working/item_feature_clean.parquet\")\n    items_info = pl.read_parquet(\"/kaggle/input/www2025-mmctr-data/MicroLens_1M_MMCTR/MicroLens_1M_x1/item_info.parquet\")\n    \n    print(f\"Items features shape: {items_features.shape}\")\n    print(f\"Items info shape: {items_info.shape}\")\n    \n    # Initialize encoder\n    print(\"Initializing CLIP encoder...\")\n    encoder = ItemEncoder(device=device)\n    \n    # Prepare for encoding\n    batch_size = 32  # Adjust based on your GPU memory\n    all_embeddings = []\n    \n    # Get item IDs in order\n    item_ids = items_features[\"item_id\"].to_list()\n    \n    print(f\"Encoding {len(item_ids)} items...\")\n    \n    # Process in batches\n    for i in tqdm(range(0, len(items_features), batch_size)):\n        batch_end = min(i + batch_size, len(items_features))\n        batch_items = items_features[i:batch_end]\n        \n        # Load images\n        images = []\n        titles = []\n        \n        for row in batch_items.iter_rows(named=True):\n            # Check if this is padding row (item_id == 0)\n            if row[\"item_id\"] == 0:\n                # For padding, we'll handle separately\n                images.append(None)\n                titles.append(\"\")\n            else:\n                try:\n                    img = Image.open(row[\"image_path\"]).convert(\"RGB\")\n                    images.append(img)\n                    # Ensure title is a string and handle None\n                    title = row[\"item_title\"] if row[\"item_title\"] else \"\"\n                    titles.append(str(title))\n                except Exception as e:\n                    print(f\"Error loading image for item {row['item_id']}: {e}\")\n                    # Use a blank image as fallback\n                    images.append(Image.new(\"RGB\", (224, 224), color=(0, 0, 0)))\n                    titles.append(row[\"item_title\"] if row[\"item_title\"] else \"\")\n        \n        # Separate padding from real items\n        non_padding_indices = [idx for idx, img in enumerate(images) if img is not None]\n        padding_indices = [idx for idx, img in enumerate(images) if img is None]\n        \n        batch_embeddings = torch.zeros((len(images), 1024), device=device)\n        \n        # Encode non-padding items\n        if non_padding_indices:\n            non_padding_images = [images[idx] for idx in non_padding_indices]\n            non_padding_titles = [titles[idx] for idx in non_padding_indices]\n            \n            embeddings = encoder.encode_items(non_padding_images, non_padding_titles)\n            \n            for local_idx, global_idx in enumerate(non_padding_indices):\n                batch_embeddings[global_idx] = embeddings[local_idx]\n        \n        # Padding rows already have zeros, so no need to set them\n        \n        all_embeddings.append(batch_embeddings.cpu().numpy())\n    \n    # Concatenate all embeddings\n    all_embeddings = np.vstack(all_embeddings)\n    print(f\"All embeddings shape: {all_embeddings.shape}\")  # Should be [91718, 1024]\n    \n    # Apply PCA to reduce from 1024 to 128 dimensions\n    print(\"Applying PCA to reduce dimensions to 128...\")\n    \n    # Separate padding and non-padding for PCA\n    padding_mask = (np.array(item_ids) == 0)\n    non_padding_embeddings = all_embeddings[~padding_mask]\n    \n    # Fit PCA on non-padding embeddings only\n    pca = PCA(n_components=128)\n    pca.fit(non_padding_embeddings)\n    \n    print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_.sum():.4f}\")\n    \n    # Transform all embeddings\n    embeddings_pca = np.zeros((len(all_embeddings), 128))\n    embeddings_pca[~padding_mask] = pca.transform(non_padding_embeddings)\n    # Padding rows remain zeros\n    \n    print(f\"PCA embeddings shape: {embeddings_pca.shape}\")  # Should be [91718, 128]\n    \n    # Convert to list of arrays for Polars\n    embeddings_list = [emb.tolist() for emb in embeddings_pca]\n    \n    # Create a mapping from item_id to embedding\n    item_id_to_embedding = dict(zip(item_ids, embeddings_list))\n    \n    # Add new column to items_info\n    print(\"Adding new column to item_info...\")\n    \n    # Map embeddings to items_info based on item_id\n    items_info_ids = items_info[\"item_id\"].to_list()\n    clip_embeddings = [item_id_to_embedding.get(iid, [0.0] * 128) for iid in items_info_ids]\n    \n    # Add the new column\n    items_info = items_info.with_columns(\n        pl.Series(\"item_clip_emb_d128\", clip_embeddings)\n    )\n    \n    print(f\"Updated items_info shape: {items_info.shape}\")\n    print(f\"Columns: {items_info.columns}\")\n    \n    # Save the updated dataframe\n    output_path = \"/kaggle/working/item_info_with_clip.parquet\"\n    items_info.write_parquet(output_path)\n    print(f\"Saved updated item_info to {output_path}\")\n    \n    # Show sample\n    print(\"\\nSample of the data:\")\n    print(items_info.head(5))\n    \n    # Verify padding row\n    padding_row = items_info.filter(pl.col(\"item_id\") == 0)\n    if len(padding_row) > 0:\n        padding_emb = padding_row[\"item_clip_emb_d128\"][0]\n        print(f\"\\nPadding embedding (first 10 values): {padding_emb[:10]}\")\n        print(f\"Padding embedding is all zeros: {all(x == 0.0 for x in padding_emb)}\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import polars as pl \n\nitems = pl.read_parquet(\"/kaggle/working/item_info_with_clip.parquet\")\nprint(items)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport polars as pl\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nimport os\n\nsns.set_style(\"whitegrid\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train = pl.read_parquet(\"/kaggle/input/www2025-mmctr-data/MicroLens_1M_MMCTR/MicroLens_1M_x1/train.parquet\")\n# valid = pl.read_parquet(\"/kaggle/input/www2025-mmctr-data/MicroLens_1M_MMCTR/MicroLens_1M_x1/valid.parquet\")\n# test = pl.read_parquet(\"/kaggle/input/www2025-mmctr-data/MicroLens_1M_MMCTR/MicroLens_1M_x1/test.parquet\")\n\n# item_info = pl.read_parquet(\"/kaggle/input/www2025-mmctr-data/MicroLens_1M_MMCTR/MicroLens_1M_x1/item_info.parquet\")\n# print(\"item info : \", item_info)\n# item_feature = pl.read_parquet(\"/kaggle/input/www2025-mmctr-data/MicroLens_1M_MMCTR/item_feature.parquet\")\n\n\n\n# selection of only features columns\n# item_feature_clean = item_feature.select([\n#     \"item_id\",\n#     \"item_title\",\n#     \"item_tags\",\n#     \"likes_level\",\n#     \"views_level\"\n# ])\n# # adding image paths \n# item_feature_clean = item_feature_clean.with_columns([\n#     (pl.lit(\"/kaggle/input/www2025-mmctr-data/MicroLens_1M_MMCTR/item_images/item_images/\") +\n#     pl.col(\"item_id\").cast(str) + \n#     pl.lit(\".jpg\")).alias(\"image_path\")\n# ])\n# # adding the padding row (idx 0)\n# padding_row = pl.DataFrame({\n#     \"item_id\":pl.Series([0], dtype=pl.Int64),\n#     \"item_title\":\"\",\n#     \"item_tags\":pl.Series([[]], dtype=pl.List(pl.Int64)),\n#     \"likes_level\":pl.Series([0], dtype=pl.Int64),\n#     \"views_level\":pl.Series([0], dtype=pl.Int64),\n#     \"image_path\": \"\"\n# })\n\n# item_feature_clean = pl.concat([padding_row, item_feature_clean])\n# # replacing the tags with padded tags from item_info\n# item_tags_padded = item_info.select([\"item_id\", \"item_tags\"])\n# item_feature_clean = item_feature_clean.drop(\"item_tags\").join(item_tags_padded, on=\"item_id\", how=\"left\")\n# print(\"item_feature_clean : \", item_feature_clean)\n\n# item_feature_clean.write_parquet(\"/kaggle/working/item_feature_clean.parquet\")\ndf = df.with_columns(pl.col(\"item_tags\").arr.to_list())\nmax = df[\"item_tags\"].list.max().max()\nprint(max)\n# item_seq = pl.read_parquet(\"/kaggle/input/www2025-mmctr-data/MicroLens_1M_MMCTR/item_seq.parquet\")\n# print(item_seq)\n# item_seq_clean = item_seq.unique(subset=[\"user_id\"], keep=\"first\").sort(\"user_id\")\n# print(\"item_seq_cleaned : \", item_seq_clean)\n# item_seq_clean.write_parquet(\"/kaggle/working/item_seq_clean.parquet\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"item_feature = pl.read_parquet(\"/kaggle/working/item_feature_clean.parquet\")\nprint(\"item_feature : \", item_feature)\n\nitem_info = pl.read_parquet(\"/kaggle/input/www2025-mmctr-data/MicroLens_1M_MMCTR/MicroLens_1M_x1/item_info.parquet\")\nprint(\"item_info : \", item_info)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n=== Label Distribution (Train) ===\")\nlabel_counts = train_with_full_seq.group_by(\"label\").agg(pl.count()).sort(\"label\")\nprint(label_counts)\n\ntotal = train_with_full_seq.shape[0]\npositive = label_counts.filter(pl.col(\"label\") == 1)[\"count\"][0]\nnegative = label_counts.filter(pl.col(\"label\") == 0)[\"count\"][0]\n\nprint(f\"\\nPositive rate: {positive/total*100:.2f}%\")\nprint(f\"Negative rate: {negative/total*100:.2f}%\")\nprint(f\"Class imbalance ratio: 1:{negative/positive:.2f}\")\n\nplt.figure(figsize=(8, 5))\nlabels = ['Negative (0)', 'Positive (1)']\ncounts = [negative, positive]\nplt.bar(labels, counts, color=['#ff6b6b', '#51cf66'])\nplt.title('Label Distribution in Training Set')\nplt.ylabel('Count')\nfor i, v in enumerate(counts):\n    plt.text(i, v, f'{v:,}\\n({v/total*100:.1f}%)', \n             ha='center', va='bottom')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n=== User Frequency Distribution ===\")\nuser_freq = train_with_full_seq.group_by(\"user_id\").agg(\n    pl.count().alias(\"user_interaction_count\")\n)\nprint(user_freq.select(pl.col(\"user_interaction_count\")).describe())\n\nprint(\"\\n=== Item Frequency Distribution ===\")\nitem_freq = train_with_full_seq.group_by(\"item_id\").agg(\n    pl.count().alias(\"item_interaction_count\")\n)\nprint(item_freq.select(pl.col(\"item_interaction_count\")).describe())\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\naxes[0].hist(user_freq[\"user_interaction_count\"], bins=50, \n             color='skyblue', edgecolor='black')\naxes[0].set_xlabel('Interactions per User')\naxes[0].set_ylabel('Number of Users')\naxes[0].set_title('User Interaction Distribution')\naxes[0].set_yscale('log')\n\n\naxes[1].hist(item_freq[\"item_interaction_count\"], bins=50, \n             color='coral', edgecolor='black')\naxes[1].set_xlabel('Interactions per Item')\naxes[1].set_ylabel('Number of Users')\naxes[1].set_title('Item Interaction Distribution')\naxes[1].set_yscale('log')\n\nplt.tight_layout()\nplt.show()\n\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n=== Item Sequence Length Analysis ===\")\n\nseq_stats = item_seq_clean.select(\n    pl.col(\"item_seq\").list.len().alias(\"original_length\")\n).describe()\nprint(seq_stats)\n\n# count non zero elements from train set\ntrain_seq_nonzero = train_with_full_seq.select(\n    pl.col(\"item_seq\").cast(pl.List(pl.Int64)).list.eval(\n        pl.element().filter(pl.element() != 0).len()\n    ).alias(\"non_zero_count\")\n)\nprint(\"\\nPadded sequences in train (non-zero items):\")\nprint(train_seq_nonzero.describe())\n\nplt.figure(figsize=(10, 5))\nseq_lengths_list = item_seq_clean.select(\n    pl.col(\"item_seq\").list.len()\n).to_numpy().flatten()\n\nplt.hist(seq_lengths_list, bins=100, color='mediumpurple', \n         edgecolor='black', alpha=0.7)\nplt.xlabel('Sequence Length')\nplt.ylabel('Number of Users')\nplt.title('Distribution of User Sequence Lengths')\nplt.axvline(seq_lengths_list.mean(), color='red', \n            linestyle='--', label=f'Mean: {seq_lengths_list.mean():.1f}')\nplt.axvline(np.median(seq_lengths_list), color='green', \n            linestyle='--', label=f'Median: {np.median(seq_lengths_list):.1f}')\nplt.legend()\nplt.tight_layout()\nplt.show()\nlong_sequences = (seq_lengths_list > 100).sum()\nprint(f\"\\n{long_sequences:,} users ({long_sequences/len(seq_lengths_list)*100:.2f}%) have >100 items (truncated in train/test)\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n=== Engagement Levels Distribution ===\")\n\n# Likes level\nprint(\"Likes Level:\")\nprint(train_with_full_seq.group_by(\"likes_level\").agg(pl.count()).sort(\"likes_level\"))\n\n# Views level\nprint(\"\\nViews Level:\")\nprint(train_with_full_seq.group_by(\"views_level\").agg(pl.count()).sort(\"views_level\"))\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nlikes_dist = train_with_full_seq.group_by(\"likes_level\").agg(pl.count()).sort(\"likes_level\")\naxes[0].bar(likes_dist[\"likes_level\"], likes_dist[\"count\"], \n            color='lightcoral')\naxes[0].set_xlabel('Likes Level')\naxes[0].set_ylabel('Count')\naxes[0].set_title('Likes Level Distribution')\n\nviews_dist = train_with_full_seq.group_by(\"views_level\").agg(pl.count()).sort(\"views_level\")\naxes[1].bar(views_dist[\"views_level\"], views_dist[\"count\"], \n            color='lightblue')\naxes[1].set_xlabel('Views Level')\naxes[1].set_ylabel('Count')\naxes[1].set_title('Views Level Distribution')\n\nplt.tight_layout()\nplt.show()\n\n# Correlation with label\nprint(\"\\n=== Engagement vs Label ===\")\nengagement_label = train_with_full_seq.group_by([\"likes_level\", \"label\"]).agg(pl.count())\nprint(engagement_label.sort([\"likes_level\", \"label\"]))","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"###########################################  Training ###########################################","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_with_full_seq = pl.read_parquet(\"/kaggle/working/train_with_full_seq.parquet\")\nvalid_with_full_seq = pl.read_parquet(\"/kaggle/working/valid_with_full_seq.parquet\")\n\nprint(train_with_full_seq)\nprint(valid_with_full_seq)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -rf /kaggle/working/recommender_CTR","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T21:39:54.265749Z","iopub.execute_input":"2025-12-20T21:39:54.266494Z","iopub.status.idle":"2025-12-20T21:39:54.477784Z","shell.execute_reply.started":"2025-12-20T21:39:54.266463Z","shell.execute_reply":"2025-12-20T21:39:54.476958Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ntoken = user_secrets.get_secret(\"github\")\n\n# Use token as username with empty password\nclone_url = f\"https://{token}:x-oauth-basic@github.com/HajarHAMDOUCH01/recommender_CTR.git\"\n!git clone $clone_url","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T21:39:54.952479Z","iopub.execute_input":"2025-12-20T21:39:54.953240Z","iopub.status.idle":"2025-12-20T21:39:56.045966Z","shell.execute_reply.started":"2025-12-20T21:39:54.953210Z","shell.execute_reply":"2025-12-20T21:39:56.045268Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'recommender_CTR'...\nremote: Enumerating objects: 264, done.\u001b[K\nremote: Counting objects: 100% (164/164), done.\u001b[K\nremote: Compressing objects: 100% (109/109), done.\u001b[K\nremote: Total 264 (delta 90), reused 124 (delta 53), pack-reused 100 (from 1)\u001b[K\nReceiving objects: 100% (264/264), 8.87 MiB | 37.70 MiB/s, done.\nResolving deltas: 100% (121/121), done.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"!python /kaggle/working/recommender_CTR/task2/model.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T21:36:25.228498Z","iopub.execute_input":"2025-12-20T21:36:25.228752Z","iopub.status.idle":"2025-12-20T21:36:28.087618Z","shell.execute_reply.started":"2025-12-20T21:36:25.228730Z","shell.execute_reply":"2025-12-20T21:36:28.086343Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"!python /kaggle/working/recommender_CTR/task2/test.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T21:39:57.910466Z","iopub.execute_input":"2025-12-20T21:39:57.910791Z","iopub.status.idle":"2025-12-20T21:41:10.093323Z","shell.execute_reply.started":"2025-12-20T21:39:57.910763Z","shell.execute_reply":"2025-12-20T21:41:10.092579Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nTEST PREDICTION SCRIPT\n======================================================================\n\nLoading test dataset...\nDataset loaded: 379,142 samples\nHas labels: False\nColumns: ['ID', 'user_id', 'item_seq', 'item_id', 'likes_level', 'views_level']\n✓ Test dataset loaded: 379,142 samples\n\n================================================================================\nLOADING ITEM DATA: item_clip_emb_d128\n================================================================================\n\nLoaded item_info shape: (91718, 4)\nColumns: ['item_id', 'item_tags', 'item_emb_d128', 'item_clip_emb_d128']\n\nLoading embeddings from: item_clip_emb_d128\nEmbeddings shape: (91718, 128)\nEmbeddings dtype: float32\nEmbeddings range: [-0.552900, 0.597740]\nEmbeddings mean: 0.000000, std: 0.064451\n\nLoading item tags...\nItem tags shape: (91718, 5)\nItem tags dtype: int64\nItem tags range: [0, 11739]\n\nNumber of items: 91,718\nNumber of unique tags: 11,740\nMax tag ID: 11739\n✓ Row 0 (item_id=0) is properly zeroed for padding\n\n================================================================================\n✓ DATA LOADED SUCCESSFULLY\n================================================================================\n\n\nInitializing model architecture...\n\n======================================================================\nTrainable params: 177,349,905\nItem embedding dim: 208\nSeq output dim: 7072\nLearning rate: 0.0005\n======================================================================\n\n\nLoading checkpoint: /kaggle/working/model_21.pth\n✓ Loaded checkpoint from epoch 21\n  Validation AUC: 0.8567\n\n======================================================================\nGENERATING TEST PREDICTIONS\n======================================================================\n\nGenerating predictions: 100%|███████████████| 1482/1482 [00:59<00:00, 24.82it/s]\n\n======================================================================\n✓ PREDICTIONS SAVED\n======================================================================\nOutput file: /kaggle/working/submission.csv\nTotal predictions: 379,142\n\nSample predictions:\n   id   Task1&2\n0   0  0.988588\n1   1  0.999792\n2   2  0.994969\n3   3  0.000008\n4   4  0.000006\n5   5  0.000011\n6   6  0.986299\n7   7  0.999962\n8   8  0.001011\n9   9  0.000479\n\nPrediction statistics:\n  Mean: 0.6072\n  Min:  0.0000\n  Max:  1.0000\n  Predictions > 0.5: 231,303 (61.01%)\n======================================================================\n\n✓ Done!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}